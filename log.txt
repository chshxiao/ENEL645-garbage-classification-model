26797
	classification layer from 1 layer to 2 layers
	2048, 256 --> relu --> 256, 4
26808
	validation loss not decrease --> over-fitting? --> reduce parameters
	classification layer
	2048, 128, --> relu --> 128, 4
26972
	validation loss much higher than training loss --> over-fitting? --> data augmentation
	add --> colorjitter
26978
	validation loss doesn't decrease, much higher --> over-fitting --> reduce parameters
	original fc layer
	2048, 128 --> relu --> 128, 4
	new fc layer
	2048, 16 --> relu --> 16, 4
26991
	same issue
	original fc layer
	2048, 16 --> relu --> 16, 4
	new fc layer
	2048 --> 4


# this test
27131
	same issue
	one-hot code
		result: cross entropy loss doesn't need one hot code

27147
	resnet 18
	fc layer
	512, 100 --> relu --> dropout 0.2 --> 100, 4
27164
	fc layer
	512, 1000 --> relu --> dropout 0.4 --> 1000, 4
27186
	fc layer
	512, 32 --> relu --> 32, 32 --> relu --> 32, 4
27187
	fc layer
	512, 16 --> relu --> 16, 8 --> relu --> 8, 4
27210
	vgg166
	last classifier layer
	4096, 4
27212
	add classifier layer
	4096, 1000 --> 1000, 4
27230
	change classifier layer
	[3] 4096, 2048
	[6] 2048, 1024
	relu --> dropout(0.5) --> 1024, 512 --> relu --> dropout(0.5) --> 512, 4
27231
	resnet50
	change fc layer
	2048, 1024 --> relu --> dropout 0.4 --> 1024, 512 --> ... --> 512, 256 --> ... --> 256, 128 --> ... --> 128, 4
	15 epochs for classifier layer + 5 epochs for fintune
27223
	same structure as 27230
	15 epochs for classifier layer learning + 5 epochs for finetune
27287
	same structure as 27212
	15 epochs for classifier layer + 5 epochs for finetune
27308
	vgg16
	1000, 100 --> relu --> dropout 0.5 --> 100, 4 v2
	15 epochs for classifier layer + 5 epochs for finetune
	batch size = 32
	gaussian blur (3, (0.1, 2))
27329
	vgg16
	4096, 2000 --> relu --> dropout 0.5 --> 2000, 4 v3
	13 epochs for classifier layer + 5 epochs for finetune
	random rotation
	random affine
27372
27443
	BERT
	original
27445
	vgg16
	4096, 1000 --> relu --> dropout 0.5 --> 1000, 2000 --> ... --> 2000, 4
27487
	BERT
	batch size 16
	save model based on accuracy
27531
	vgg16
	change input size to 256,256
	avgpool layer --> output size 8, 8
27605
	vgg16
	last two layers activated
	4096, 1000 --> 1000, 4
	weighted cross entropy loss

possible solutions
data augmentation
unfreeze some other layers
increase batch size

early stopping
https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/
tricks for transfer learning
https://arxiv.org/pdf/1812.01187

